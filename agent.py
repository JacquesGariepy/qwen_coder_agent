import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging
import json
import re

# Configure logging to see what is happening
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class CoderAgent:
    """
    Small agent responsible for generating code using a specified LLM model.
    """
    def __init__(self, model_name: str):
        """
        Initializes the CoderAgent by loading the model and tokenizer.
        Args:
            model_name (str): Name or path of the Hugging Face model to load.
        """
        self.model_name = model_name
        logging.info(f"Initializing CoderAgent with model: {self.model_name}")
        try:
            # Detect if a GPU is available, otherwise use the CPU
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logging.info(f"Using device: {self.device}")

            # Use device_map="auto" to let transformers manage placement (CPU/GPU/multi-GPU)
            # Use torch_dtype="auto" to choose the optimal data type (e.g., bfloat16 on recent GPUs)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype="auto",
                device_map="auto"  # Very important for efficient resource allocation
            )
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            logging.info("Model and tokenizer successfully loaded.")

        except Exception as e:
            logging.error(f"Error while loading the model or tokenizer: {e}")
            # You might want to stop the program here or handle the error differently
            raise  # Propagate the exception

    def generate_code(self, task_prompt: str, max_new_tokens: int = 512) -> str:
        """
        Generates code based on the requested task.
        Args:
            task_prompt (str): The description of the coding task.
            max_new_tokens (int): The maximum number of new tokens to generate.
        Returns:
            str: The code snippet generated by the model.
        """
        logging.info(f"CoderAgent received the task: '{task_prompt}'")
        
        # Enhance the prompt to ensure we get only code without text
        enhanced_task = f"""
{task_prompt}

IMPORTANT: 
- Provide ONLY the code implementation.
- Do NOT include any text explanations, comments outside the code, or markdown blocks.
- Do NOT write any text before or after the code.
- Only provide the raw, executable Python code.
"""
        
        messages = [
            {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a coding assistant that provides ONLY code - no explanations, no markdown, no text, just pure executable code."},
            {"role": "user", "content": enhanced_task}
        ]
        try:
            # Prepare the prompt for the model using the chat template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True  # Important for instruct/chat models
            )

            # Tokenize the text and send tensors to the correct device
            # The device is determined by device_map="auto" during loading
            # Use self.model.device to ensure inputs are on the same device as the model
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

            logging.info("Generating code...")
            # Generate the model's response
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                # Other possible generation parameters: temperature, top_k, do_sample...
                pad_token_id=self.tokenizer.eos_token_id  # Often useful to avoid warnings
            )

            # Decode the generated response
            # Remove the tokens of the initial prompt to keep only the response
            input_token_len = model_inputs.input_ids.shape[1]
            # Ensure only the newly generated tokens are taken
            generated_ids_only = generated_ids[:, input_token_len:]

            response = self.tokenizer.batch_decode(generated_ids_only, skip_special_tokens=True)[0]

            logging.info("Code generation completed.")
            # Clean up the response (remove extra spaces at the beginning/end)
            response = response.strip()

            # Extract code if it's within markdown code blocks
            response = self.extract_code_from_markdown(response)

            return response

        except Exception as e:
            logging.error(f"Error during code generation: {e}")
            return f"Error during generation: {e}"
            
    def extract_code_from_markdown(self, text: str) -> str:
        """
        Extract code from markdown code blocks, or return the original text if no blocks are found.
        Handles cases where code is wrapped in ```python...``` or just ```...``` blocks.
        
        Args:
            text (str): The text that may contain markdown code blocks
            
        Returns:
            str: The extracted code or the original text if no code blocks are found
        """
        import re
        
        # Try to extract code from ```python...``` blocks
        python_pattern = r"```python\s*([\s\S]*?)\s*```"
        python_matches = re.findall(python_pattern, text)
        
        if python_matches:
            # Join multiple python code blocks if they exist
            return "\n\n".join(match.strip() for match in python_matches)
        
        # Try to extract code from generic ```...``` blocks
        generic_pattern = r"```\s*([\s\S]*?)\s*```"
        generic_matches = re.findall(generic_pattern, text)
        
        if generic_matches:
            # Join multiple code blocks if they exist
            return "\n\n".join(match.strip() for match in generic_matches)
        
        # If no code blocks are found, return the original text
        # This might be pure code already
        return text


class ManagerAgent:
    """
    Larger agent that assigns tasks to the CoderAgent and can validate the results using the same LLM.
    """
    def __init__(self, model_name: str, coder_agent: CoderAgent = None):
        """
        Initializes the ManagerAgent.
        Args:
            model_name (str): Name or path of the Hugging Face model to load.
            coder_agent (CoderAgent, optional): An instance of the CoderAgent to delegate tasks to.
                                               If None, will create a new CoderAgent with the provided model.
        """
        logging.info("Initializing ManagerAgent.")
        
        # Create a new CoderAgent if none is provided
        if coder_agent is None:
            logging.info(f"Creating a new CoderAgent with model: {model_name}")
            self.coder_agent = CoderAgent(model_name=model_name)
        else:
            if not isinstance(coder_agent, CoderAgent):
                raise TypeError("coder_agent must be an instance of CoderAgent")
            self.coder_agent = coder_agent
            
        # Set up model and tokenizer for validation tasks
        try:
            # Use the same model for validation to avoid loading twice when possible
            if coder_agent is not None:
                self.model = coder_agent.model
                self.tokenizer = coder_agent.tokenizer
                logging.info("Using existing model from CoderAgent for validation.")
            else:
                # Detect if a GPU is available, otherwise use the CPU
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
                logging.info(f"Using device for manager: {self.device}")

                # Load the model and tokenizer for validation
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype="auto",
                    device_map="auto"
                )
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                logging.info("Manager model and tokenizer successfully loaded.")
                
        except Exception as e:
            logging.error(f"Error while loading the model or tokenizer for manager: {e}")
            raise

    def assign_task(self, task_description: str) -> str:
        """
        Assigns a task to the CoderAgent and returns the result.
        Args:
            task_description (str): The description of the coding task.
        Returns:
            str: The code generated by the CoderAgent.
        """
        logging.info(f"ManagerAgent assigns the task: '{task_description}' to the CoderAgent.")
        generated_code = self.coder_agent.generate_code(task_description)
        logging.info("ManagerAgent received the result from the CoderAgent.")
        return generated_code

    def validate_code_basic(self, code: str, task_description: str) -> bool:
        """
        Performs basic validation on the generated code.
        Args:
            code (str): The generated code to validate.
            task_description (str): The original task description.
        Returns:
            bool: True if basic validation passes, False otherwise.
        """
        logging.info("ManagerAgent starts basic validation of the code.")
        if not code or code.startswith("Error"):
            logging.warning(f"Basic validation FAILED: The code is empty or an error occurred.")
            return False

        # --- Basic validation logic ---
        is_valid = True
        # Example 1: Check if the code contains expected keywords
        if "quick sort" in task_description.lower():
            if "def" not in code or "pivot" not in code or "sort" not in code.lower():
                 logging.warning("Basic validation: The code for Quick Sort seems to lack key elements.")
                 is_valid = False

        # Example 2: Try to compile the code (beware of imports and execution)
        try:
            compile(code, "<string>", "exec")
            logging.info("Basic validation: The code compiles (valid Python syntax).")
        except SyntaxError as e:
            logging.warning(f"Basic validation FAILED: Syntax error - {e}")
            is_valid = False

        return is_valid

    def validate_code_with_llm(self, code: str, task_description: str, max_new_tokens: int = 256) -> tuple:
        """
        Validates the generated code using the LLM.
        Args:
            code (str): The generated code to validate.
            task_description (str): The description of the original task.
            max_new_tokens (int): The maximum number of new tokens to generate.
        Returns:
            tuple: (bool, str) - (True/False, feedback/reason)
        """
        logging.info("ManagerAgent starts validating the code using the LLM.")
        
        # First perform basic validation
        if not self.validate_code_basic(code, task_description):
            return False, "Failed basic validation checks (syntax, etc.)"
        
        # Prepare the prompt for validation
        validation_prompt = f"""
You are a code validation expert. Evaluate the following Python code that was generated for this task:

Task description: {task_description}

Code to evaluate:
```python
{code}
```

Respond with a single JSON object only, like this:
{{
  "valid": true or false,
  "feedback": "Your specific feedback about the code"
}}

Does the code accurately implement the requested functionality? Is it properly written? Are there any bugs, errors, or edge cases not handled?
Remember to respond ONLY with the JSON object and nothing else.
"""
        
        messages = [
            {"role": "system", "content": "You are Qwen, a precise code validation assistant. You respond only with JSON."},
            {"role": "user", "content": validation_prompt}
        ]
        
        try:
            # Prepare the prompt for the model using the chat template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # Tokenize the text and send tensors to the correct device
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

            logging.info("Generating validation response...")
            # Generate the model's response
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id
            )

            # Decode the generated response
            input_token_len = model_inputs.input_ids.shape[1]
            generated_ids_only = generated_ids[:, input_token_len:]
            response = self.tokenizer.batch_decode(generated_ids_only, skip_special_tokens=True)[0]
            response = response.strip()
            
            logging.info(f"Validation response from LLM: {response}")
            
            # Parse the response - using regex to extract JSON if needed
            import json
            import re
            
            # Try to extract JSON object using regex if necessary
            json_pattern = r'{[\s\S]*}'
            matches = re.findall(json_pattern, response)
            
            if matches:
                json_text = matches[0]
            else:
                json_text = response
                
            try:
                result = json.loads(json_text)
                is_valid = result.get("valid", False)
                feedback = result.get("feedback", "No specific feedback provided")
                
                # Check for contradictions in feedback
                # If feedback contains negative words but is marked as valid, double-check
                negative_terms = ["incorrect", "not correct", "wrong", "error", "fail", "issue", 
                                 "bug", "problem", "missing", "doesn't work", "not working"]
                
                if is_valid and any(term in feedback.lower() for term in negative_terms):
                    logging.warning(f"Validation contradiction detected! Marked as valid but feedback contains negative terms: {feedback}")
                    # Override the validation result
                    is_valid = False
                
                logging.info(f"Validation result: {'Passed' if is_valid else 'Failed'} - {feedback}")
                return is_valid, feedback
                
            except json.JSONDecodeError as e:
                logging.warning(f"Error parsing validation response as JSON: {e}")
                # Fallback interpretation
                is_valid = '"valid": true' in response.lower() or '"valid":true' in response.lower()
                return is_valid, "Could not parse detailed feedback from LLM response."
                
        except Exception as e:
            logging.error(f"Error during LLM validation: {e}")
            return False, f"Error during validation: {e}"

    def feedback_loop(self, task_description: str, max_loops: int = 3) -> tuple:
        """
        Implements a feedback loop between the ManagerAgent and the CoderAgent.
        If the code is not validated, the ManagerAgent provides feedback to the CoderAgent.

        Args:
            task_description (str): The description of the coding task.
            max_loops (int): Maximum number of feedback iterations to avoid infinite loops.

        Returns:
            tuple: (final_code, validation_history) - The final code and a list of validation messages
                   IMPORTANT: Keep the original return signature for backward compatibility
        """
        logging.info(f"Starting feedback loop for task: '{task_description}' with max_loops={max_loops}")
        
        validation_history = []
        current_task = task_description
        
        for loop_count in range(max_loops):
            logging.info(f"Feedback loop iteration: {loop_count + 1}")

            # Generate code using the CoderAgent
            generated_code = self.coder_agent.generate_code(current_task)
            
            # Validate the generated code using the LLM
            is_valid, feedback = self.validate_code_with_llm(generated_code, task_description)
            
            # Record validation results
            validation_entry = {
                "iteration": loop_count + 1,
                "valid": is_valid,
                "feedback": feedback,
                "code": generated_code
            }
            validation_history.append(validation_entry)
            
            # If code is valid, we're done
            if is_valid:
                logging.info(f"Code validated successfully on iteration {loop_count + 1}.")
                # Return only the two values for backward compatibility
                return generated_code, validation_history

            # Update the task description with feedback for the next iteration
            current_task = f"""
{task_description}

PREVIOUS CODE HAD ISSUES:
{feedback}

IMPORTANT: 
1. Fix the issues mentioned above
2. Provide ONLY the corrected code implementation
3. Do NOT include any text explanations or markdown blocks
4. Do NOT write any text before or after the code
5. Only provide the raw, executable Python code
"""
            logging.info(f"Updating task with feedback for next iteration.")

        # We've reached max iterations
        logging.warning("Maximum feedback loop iterations reached. Returning the last generated code.")
        # Return only the two values for backward compatibility
        return generated_code, validation_history


# --- Main entry point ---
if __name__ == "__main__":
    import argparse
    import sys
    import os

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Generate and validate code using Qwen LLM")
    
    # Task selection options (mutually exclusive group)
    task_group = parser.add_mutually_exclusive_group()
    task_group.add_argument("--task", "-t", type=str, help="Single task description for code generation")
    task_group.add_argument("--tasks", type=str, nargs="+", help="Multiple task descriptions to process sequentially")
    task_group.add_argument("--tasks-file", type=str, help="Path to JSON file containing task descriptions")
    
    parser.add_argument("--model", "-m", type=str, default="Qwen/Qwen2.5-Coder-0.5B-Instruct", 
                        help="Model name or path (default: Qwen/Qwen2.5-Coder-0.5B-Instruct)")
    parser.add_argument("--loops", "-l", type=int, default=3, 
                        help="Maximum number of feedback loops (default: 3)")
    parser.add_argument("--output-dir", "-o", type=str, default="generated_code",
                        help="Directory to save generated code (default: 'generated_code')")
    args = parser.parse_args()

    # Name of the model to use
    MODEL_NAME = args.model

    # Define sample tasks
    sample_tasks = [
        "Write a Python function for quick sort algorithm.",
        "Create a simple Python function that adds two numbers. Provide only the code.",
        "Write a basic Flask app with a single route '/' that returns 'Hello World!'."
    ]

    # Determine the tasks to process
    tasks_to_process = []
    
    # Read from tasks file if provided
    if args.tasks_file:
        try:
            with open(args.tasks_file, 'r') as f:
                file_content = f.read().strip()
                if file_content:
                    if file_content.startswith('['):
                        # Try to parse as JSON array
                        tasks_to_process = json.loads(file_content)
                    else:
                        # Each line is a task
                        tasks_to_process = [line.strip() for line in file_content.split('\n') if line.strip()]
            print(f"Loaded {len(tasks_to_process)} tasks from {args.tasks_file}")
        except (json.JSONDecodeError, IOError) as e:
            print(f"Error reading tasks file: {e}")
            sys.exit(1)
    
    # Use task arguments if provided
    elif args.tasks:
        tasks_to_process = args.tasks
        print(f"Processing {len(tasks_to_process)} tasks from command line arguments")
    
    elif args.task:
        tasks_to_process = [args.task]
    
    # If no tasks provided, use interactive mode
    else:
        # Interactive selection
        print("No tasks provided. Please choose one of the following options:")
        print("\n1. Run a sample task")
        print("2. Enter a custom task")
        print("3. Enter multiple custom tasks")
        
        choice = input("\nEnter your choice (1-3): ")
        
        if choice == "1":
            print("\nSample tasks:")
            for i, task in enumerate(sample_tasks):
                print(f"{i+1}. {task}")
            
            choice_num = int(input("\nEnter task number: "))
            if 1 <= choice_num <= len(sample_tasks):
                tasks_to_process = [sample_tasks[choice_num-1]]
            else:
                print("Invalid choice.")
                sys.exit(1)
                
        elif choice == "2":
            task = input("\nEnter your custom task description: ")
            tasks_to_process = [task]
            
        elif choice == "3":
            print("\nEnter your custom tasks, one per line. Enter an empty line to finish.")
            tasks = []
            while True:
                task = input("> ")
                if not task:
                    break
                tasks.append(task)
            
            if tasks:
                tasks_to_process = tasks
            else:
                print("No tasks entered.")
                sys.exit(1)
        else:
            print("Invalid choice.")
            sys.exit(1)
    
    # Create output directory if it doesn't exist
    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)
        print(f"Created output directory: {args.output_dir}")

    try:
        # Create the manager agent
        manager_agent = ManagerAgent(model_name=MODEL_NAME)
        
        # Process each task
        task_results = []
        for task_idx, task in enumerate(tasks_to_process):
            task_num = task_idx + 1
            total_tasks = len(tasks_to_process)
            
            print(f"\n{'='*80}")
            print(f"Task {task_num}/{total_tasks}:")
            print(f"{task}")
            print(f"{'='*80}\n")
            
            # Execute the feedback loop
            print(f"Starting feedback loop (max {args.loops} iterations)...")
            # Use the original function signature (2 return values)
            final_code, validation_history = manager_agent.feedback_loop(task, max_loops=args.loops)
            
            # Determine validity from the last entry
            is_valid = validation_history[-1]['valid'] if validation_history else False
            
            # Record result for this task
            task_results.append({
                "task": task,
                "code": final_code,
                "valid": is_valid,
                "iterations": len(validation_history)
            })
            
            # Display the process and results
            print("\n--- Feedback Loop Process ---")
            for i, entry in enumerate(validation_history):
                print(f"\nIteration {i+1}:")
                print(f"Valid: {entry['valid']}")
                print(f"Feedback: {entry['feedback']}")
                print("Code:")
                print("-" * 40)
                print(entry['code'])
                print("-" * 40)
            
            print("\n--- Final Code ---")
            print(final_code)
            print("-" * 40)
            print(f"Validation status: {'✅ PASSED' if is_valid else '❌ FAILED'}")
            
            # Only save and execute if code is valid
            if is_valid:
                # Save the code to a file
                task_name = task.split()[0:5]  # Use first 5 words of task for filename
                task_name = "_".join([word.lower() for word in task_name if word.isalnum()])
                task_name = task_name[:50]  # Limit length
                
                filename = f"task_{task_num:02d}_{task_name}.py"
                file_path = os.path.join(args.output_dir, filename)
                
                with open(file_path, 'w') as f:
                    f.write(f"# Task: {task}\n\n")
                    f.write(final_code)
                
                print(f"\nSaved valid code to: {file_path}")
                
                # Execute the code
                print("\nExecuting the final code...\n")
                try:
                    # This could be dangerous with untrusted code!
                    # For demonstration purposes only
                    print("Output of code execution:")
                    
                    # Create an isolated scope to execute the code
                    local_vars = {}
                    exec(final_code, {}, local_vars)
                    
                    # For the "add two numbers" example, test the function
                    if "add" in local_vars:
                        result = local_vars["add"](5, 7)
                        print(f"Testing add(5, 7) = {result}")
                        
                except Exception as e:
                    print(f"Error executing the code: {e}")
            else:
                print("\n⚠️ Code validation failed - no file saved and code not executed.")
        
        # Summarize all tasks
        print(f"\n{'='*80}")
        print(f"SUMMARY OF ALL TASKS")
        print(f"{'='*80}")
        
        valid_count = sum(1 for r in task_results if r["valid"])
        print(f"Tasks processed: {len(task_results)}")
        print(f"Tasks with valid code: {valid_count}")
        print(f"Success rate: {valid_count/len(task_results)*100:.2f}%")
        
        print("\nTask details:")
        for i, result in enumerate(task_results):
            print(f"{i+1}. {result['task'][:50]}{'...' if len(result['task']) > 50 else ''}")
            print(f"   - Status: {'✅ VALID' if result['valid'] else '❌ INVALID'}")
            print(f"   - Iterations: {result['iterations']}")
            
        print(f"\nCompleted all {len(tasks_to_process)} tasks!")

    except Exception as e:
        logging.error(f"A major error occurred in the main execution: {e}")
        print(f"\nERROR: Unable to execute the script. Check the logs and your setup. Error: {e}")
