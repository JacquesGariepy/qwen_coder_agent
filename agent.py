import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

# Configure logging to see what is happening
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class CoderAgent:
    """
    Small agent responsible for generating code using a specified LLM model.
    """
    def __init__(self, model_name: str):
        """
        Initializes the CoderAgent by loading the model and tokenizer.
        Args:
            model_name (str): Name or path of the Hugging Face model to load.
        """
        self.model_name = model_name
        logging.info(f"Initializing CoderAgent with model: {self.model_name}")
        try:
            # Detect if a GPU is available, otherwise use the CPU
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logging.info(f"Using device: {self.device}")

            # Use device_map="auto" to let transformers manage placement (CPU/GPU/multi-GPU)
            # Use torch_dtype="auto" to choose the optimal data type (e.g., bfloat16 on recent GPUs)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype="auto",
                device_map="auto"  # Very important for efficient resource allocation
            )
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            logging.info("Model and tokenizer successfully loaded.")

        except Exception as e:
            logging.error(f"Error while loading the model or tokenizer: {e}")
            # You might want to stop the program here or handle the error differently
            raise  # Propagate the exception

    def generate_code(self, task_prompt: str, max_new_tokens: int = 512) -> str:
        """
        Generates code based on the requested task.
        Args:
            task_prompt (str): The description of the coding task.
            max_new_tokens (int): The maximum number of new tokens to generate.
        Returns:
            str: The code snippet generated by the model.
        """
        logging.info(f"CoderAgent received the task: '{task_prompt}'")
        messages = [
            {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful coding assistant. No talk, just code."},
            {"role": "user", "content": task_prompt}
        ]
        try:
            # Prepare the prompt for the model using the chat template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True  # Important for instruct/chat models
            )

            # Tokenize the text and send tensors to the correct device
            # The device is determined by device_map="auto" during loading
            # Use self.model.device to ensure inputs are on the same device as the model
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

            logging.info("Generating code...")
            # Generate the model's response
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                # Other possible generation parameters: temperature, top_k, do_sample...
                pad_token_id=self.tokenizer.eos_token_id  # Often useful to avoid warnings
            )

            # Decode the generated response
            # Remove the tokens of the initial prompt to keep only the response
            input_token_len = model_inputs.input_ids.shape[1]
            # Ensure only the newly generated tokens are taken
            generated_ids_only = generated_ids[:, input_token_len:]

            response = self.tokenizer.batch_decode(generated_ids_only, skip_special_tokens=True)[0]

            logging.info("Code generation completed.")
            # Clean up the response (remove extra spaces at the beginning/end)
            response = response.strip()

            # Remove ```python and ``` tags if present
            if response.startswith("```python"):
                response = response[len("```python"):].strip()
            if response.endswith("```"):
                response = response[:-len("```")].strip()

            return response

        except Exception as e:
            logging.error(f"Error during code generation: {e}")
            return f"Error during generation: {e}"


class ManagerAgent:
    """
    Larger agent that assigns tasks to the CoderAgent and can validate the results using the same LLM.
    """
    def __init__(self, model_name: str, coder_agent: CoderAgent = None):
        """
        Initializes the ManagerAgent.
        Args:
            model_name (str): Name or path of the Hugging Face model to load.
            coder_agent (CoderAgent, optional): An instance of the CoderAgent to delegate tasks to.
                                               If None, will create a new CoderAgent with the provided model.
        """
        logging.info("Initializing ManagerAgent.")
        
        # Create a new CoderAgent if none is provided
        if coder_agent is None:
            logging.info(f"Creating a new CoderAgent with model: {model_name}")
            self.coder_agent = CoderAgent(model_name=model_name)
        else:
            if not isinstance(coder_agent, CoderAgent):
                raise TypeError("coder_agent must be an instance of CoderAgent")
            self.coder_agent = coder_agent
            
        # Set up model and tokenizer for validation tasks
        try:
            # Use the same model for validation to avoid loading twice when possible
            if coder_agent is not None:
                self.model = coder_agent.model
                self.tokenizer = coder_agent.tokenizer
                logging.info("Using existing model from CoderAgent for validation.")
            else:
                # Detect if a GPU is available, otherwise use the CPU
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
                logging.info(f"Using device for manager: {self.device}")

                # Load the model and tokenizer for validation
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype="auto",
                    device_map="auto"
                )
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                logging.info("Manager model and tokenizer successfully loaded.")
                
        except Exception as e:
            logging.error(f"Error while loading the model or tokenizer for manager: {e}")
            raise

    def assign_task(self, task_description: str) -> str:
        """
        Assigns a task to the CoderAgent and returns the result.
        Args:
            task_description (str): The description of the coding task.
        Returns:
            str: The code generated by the CoderAgent.
        """
        logging.info(f"ManagerAgent assigns the task: '{task_description}' to the CoderAgent.")
        generated_code = self.coder_agent.generate_code(task_description)
        logging.info("ManagerAgent received the result from the CoderAgent.")
        return generated_code

    def validate_code_basic(self, code: str, task_description: str) -> bool:
        """
        Performs basic validation on the generated code.
        Args:
            code (str): The generated code to validate.
            task_description (str): The original task description.
        Returns:
            bool: True if basic validation passes, False otherwise.
        """
        logging.info("ManagerAgent starts basic validation of the code.")
        if not code or code.startswith("Error"):
            logging.warning(f"Basic validation FAILED: The code is empty or an error occurred.")
            return False

        # --- Basic validation logic ---
        is_valid = True
        # Example 1: Check if the code contains expected keywords
        if "quick sort" in task_description.lower():
            if "def" not in code or "pivot" not in code or "sort" not in code.lower():
                 logging.warning("Basic validation: The code for Quick Sort seems to lack key elements.")
                 is_valid = False

        # Example 2: Try to compile the code (beware of imports and execution)
        try:
            compile(code, "<string>", "exec")
            logging.info("Basic validation: The code compiles (valid Python syntax).")
        except SyntaxError as e:
            logging.warning(f"Basic validation FAILED: Syntax error - {e}")
            is_valid = False

        return is_valid

    def validate_code_with_llm(self, code: str, task_description: str, max_new_tokens: int = 256) -> tuple:
        """
        Validates the generated code using the LLM.
        Args:
            code (str): The generated code to validate.
            task_description (str): The description of the original task.
            max_new_tokens (int): The maximum number of new tokens to generate.
        Returns:
            tuple: (bool, str) - (True/False, feedback/reason)
        """
        logging.info("ManagerAgent starts validating the code using the LLM.")
        
        # First perform basic validation
        if not self.validate_code_basic(code, task_description):
            return False, "Failed basic validation checks (syntax, etc.)"
        
        # Prepare the prompt for validation
        validation_prompt = f"""
You are a code validation expert. Evaluate the following Python code based on these criteria:
1. Does it correctly implement the requested task?
2. Is it efficient and follows best practices?
3. Are there any bugs or edge cases not handled?

Task description: {task_description}

Code to evaluate:
```python
{code}
```

Provide your evaluation as a JSON with two fields:
- "valid": true/false (is the code acceptable?)
- "feedback": a brief explanation of issues found or confirmation it's good

Only respond with the JSON object, no additional text.
"""
        
        messages = [
            {"role": "system", "content": "You are Qwen, a helpful code validation assistant."},
            {"role": "user", "content": validation_prompt}
        ]
        
        try:
            # Prepare the prompt for the model using the chat template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # Tokenize the text and send tensors to the correct device
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

            logging.info("Generating validation response...")
            # Generate the model's response
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id
            )

            # Decode the generated response
            input_token_len = model_inputs.input_ids.shape[1]
            generated_ids_only = generated_ids[:, input_token_len:]
            response = self.tokenizer.batch_decode(generated_ids_only, skip_special_tokens=True)[0]
            response = response.strip()
            
            logging.info(f"Validation response from LLM: {response}")
            
            # Parse the response - note: this is simplified and might need more robust parsing
            import json
            try:
                # Try to extract JSON if it's wrapped in ```json ... ``` or other markers
                if "```json" in response:
                    json_text = response.split("```json")[1].split("```")[0].strip()
                elif "```" in response:
                    json_text = response.split("```")[1].strip()
                else:
                    json_text = response
                
                result = json.loads(json_text)
                is_valid = result.get("valid", False)
                feedback = result.get("feedback", "No specific feedback provided")
                
                logging.info(f"Validation result: {'Passed' if is_valid else 'Failed'} - {feedback}")
                return is_valid, feedback
                
            except (json.JSONDecodeError, IndexError) as e:
                logging.warning(f"Error parsing validation response: {e}")
                # Fallback interpretation
                is_valid = '"valid": true' in response.lower() or '"valid":true' in response.lower()
                return is_valid, "Could not parse detailed feedback from LLM response."
                
        except Exception as e:
            logging.error(f"Error during LLM validation: {e}")
            return False, f"Error during validation: {e}"

    def feedback_loop(self, task_description: str, max_loops: int = 3) -> tuple:
        """
        Implements a feedback loop between the ManagerAgent and the CoderAgent.
        If the code is not validated, the ManagerAgent provides feedback to the CoderAgent.

        Args:
            task_description (str): The description of the coding task.
            max_loops (int): Maximum number of feedback iterations to avoid infinite loops.

        Returns:
            tuple: (final_code, validation_history) - The final code and a list of validation messages
        """
        logging.info(f"Starting feedback loop for task: '{task_description}' with max_loops={max_loops}")
        
        validation_history = []
        current_task = task_description
        
        for loop_count in range(max_loops):
            logging.info(f"Feedback loop iteration: {loop_count + 1}")

            # Generate code using the CoderAgent
            generated_code = self.coder_agent.generate_code(current_task)
            
            # Validate the generated code using the LLM
            is_valid, feedback = self.validate_code_with_llm(generated_code, task_description)
            
            # Record validation results
            validation_entry = {
                "iteration": loop_count + 1,
                "valid": is_valid,
                "feedback": feedback,
                "code": generated_code
            }
            validation_history.append(validation_entry)
            
            # If code is valid, we're done
            if is_valid:
                logging.info(f"Code validated successfully on iteration {loop_count + 1}.")
                return generated_code, validation_history

            # Update the task description with feedback for the next iteration
            current_task = f"""
{task_description}

Your previous solution had issues:
{feedback}

Please fix these issues and provide an improved solution:
"""
            logging.info(f"Updating task with feedback for next iteration.")

        # Return the last generated code if we reached max iterations
        logging.warning("Maximum feedback loop iterations reached. Returning the last generated code.")
        return generated_code, validation_history


# --- Main entry point ---
if __name__ == "__main__":
    # Name of the model to use
    MODEL_NAME = "Qwen/Qwen2.5-Coder-0.5B-Instruct"

    try:
        # 1. Create the manager agent (which will create a coder agent internally)
        manager_agent = ManagerAgent(model_name=MODEL_NAME)

        # 2. Define sample tasks
        tasks = [
            "Write a Python function for quick sort algorithm.",
            "Create a simple Python function that adds two numbers. Provide only the code.",
            "Write a basic Flask app with a single route '/' that returns 'Hello World!'."
        ]

        # 3. Choose a task to execute
        task_to_perform = tasks[1]  # Change the index to try different tasks
        
        # 4. Execute the feedback loop
        final_code, validation_history = manager_agent.feedback_loop(task_to_perform)
        
        # 5. Display the process and results
        print("\n--- Feedback Loop Process ---")
        for i, entry in enumerate(validation_history):
            print(f"\nIteration {i+1}:")
            print(f"Valid: {entry['valid']}")
            print(f"Feedback: {entry['feedback']}")
            print("Code:")
            print("-" * 40)
            print(entry['code'])
            print("-" * 40)
        
        print("\n--- Final Code ---")
        print(final_code)
        print("-" * 40)
        
        # 6. Execute the code if it passed validation (optional)
        if validation_history[-1]['valid']:
            print("\nExecuting the final code...\n")
            try:
                # This could be dangerous with untrusted code!
                # For demonstration purposes only
                print("Output of code execution:")
                
                # Create an isolated scope to execute the code
                local_vars = {}
                exec(final_code, {}, local_vars)
                
                # For the "add two numbers" example, test the function
                if "add" in local_vars:
                    result = local_vars["add"](5, 7)
                    print(f"Testing add(5, 7) = {result}")
                    
            except Exception as e:
                print(f"Error executing the code: {e}")
        
    except Exception as e:
        logging.error(f"A major error occurred in the main execution: {e}")
        print(f"\nERROR: Unable to execute the script. Check the logs and your setup. Error: {e}")
